<!DOCTYPE html>
<html lang="en">

<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Shunli Wang - Doctor Candidate - Fudan University</title>
    <link href="css/style.css" rel="stylesheet">
    <link href="css/header.css" rel="stylesheet">
    <link href="css/publications.css" rel="stylesheet">
    <link href="css/projects.css" rel="stylesheet">
    <link href="css/timeline.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Asap:400,400i,500" rel="stylesheet" type='text/css'>
    <style type="text/css"></style>

    <script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.0/jquery.min.js"></script>
    <script type="text/javascript">
        $(document).ready(function(){
            $(".trigger").click(function(){ // trigger
                $(this).nextAll(".abstract").slideToggle("fast");
                $(this).children("a").toggleClass("closed open");
            });
        });
    </script>

</head>

<body>

<!-- HEADER -->
<div class="header">
    <div class="headermain">
        <div class="pubimg"><img src="img/ShunliWang.png" width="110" height="110" class="profilepic"></div>
        <div style="display: table-row">
            <div class="name">Shunli Wang (王顺利)</div>
            <div class="affiliation">Ph.D. Student at Fudan University, Shanghai</div>
            <div class="contact">slwang19@fudan.edu.cn</div>
        </div>
    </div>
    <div class="menu">
        <table style="width: 100%;">
            <tr>
                <th class="menuentry" style="width: 25%;"><a href="#publications">Publications</a></th>
                <th class="menuentry" style="width: 25%;"><a href="#projects">Projects</a></th>
                <th class="menuentry" style="width: 25%;"><a href="#education">Education/Work</a></th>
            </tr>
        </table>
    </div>
</div>

<!-- CONTENT -->
<div class="main">
    I am a third-year Ph.D. student at Academy for Engineering and Technology of Fudan University. 
    As a part of the Intelligent Perception Laboratory, I am advised by Prof. <a href="http://faet.fudan.edu.cn/e4/71/c23902a255089/page.htm" style="color: blue;"> Lihua Zhang </a>.
    Before that, I received my B.S. degree from School of Electrical Engineering and Automation, Anhui University, in 2019.
    My research interests are <b>vision-based fine-grained action recognition</b> and <b>action quality assessment</b> in medical scenes.
    I have also explored many vision tasks such as multi-target tracking, network quantization, human-object interaction detection and 6D pose estimation of satellites.

    <!-- PUBLICATIONS -->
    <a name="publications">
    <div class="headline">Publications</div>

    <!-- publication -->
    <div class="publication">
        <div class="pubimg"><img src="publications/img/slwang_TSA-Net.png" class="thumbnail"></div>
        <div style="display:table-row">
            <div class="pubtitle">TSA-Net: Tube Self-Attention Network for Action Quality Assessment </div>
            <div class="pubauthors"><b>Shunli Wang</b>, Dingkang Yang, Peng Zhai, Chixiao Chen, Lihua Zhang*</div>
            <div class="pubinfo">ACM International Conference on Multimedia (ACM-MM 2021) (Oral) </div>
            <div class="pubbox trigger"><a href="javascript:void(0)">Abstract</a></div>
            <div class="pubbox"><a href="publications/pdf/slwang_TSA-Net.pdf">PDF</a></div>
            <div class="pubbox"><a href=https://github.com/Shunli-Wang/TSA-Net>Code</a></div>
            <div class="pubbox"><a href=https://youtu.be/WdAVw1r-fno>Oral</a></div>
            <div class="pubbox"><a href="publications/pdf/slwang_TSA-Net_supp.pdf">Supp</a></div>
            <div class="pubbox"><a href="publications/pdf/slwang_TSA-Net_slides.pdf">Slides</a></div>
            <div class="pubbox"><a href="publications/pdf/slwang_TSA-Net_poster.pdf">Poster</a></div>
            <div class="pubbox"><a href="publications/bibtex/slwang_TSA-Net.bib">BibTex</a></div>
            <div class="abstract">In recent years, assessing action quality from videos has attracted growing attention in computer vision community and human-computer interaction. 
                Most existing approaches usually tackle this problem by directly migrating the model from action recognition tasks, which ignores the intrinsic differences within the 
                feature map such as foreground and background information. To address this issue, we propose a Tube Self-Attention Network (TSA-Net) for action quality assessment (AQA). 
                Specifically, we introduce a single object tracker into AQA and propose the Tube Self-Attention Module (TSA), which can efficiently generate rich spatio-temporal contextual 
                information by adopting sparse feature interactions. The TSA module is embedded in existing video networks to form TSA-Net. Overall, our TSA-Net is with the following merits: 1) 
                High computational efficiency, 2) High flexibility, and 3) The state-of-the-art performance. Extensive experiments are conducted on popular action quality assessment datasets 
                including AQA-7 and MTL-AQA. Besides, a dataset named Fall Recognition in Figure Skating (FR-FS) is proposed to explore the basic action assessment in the figure skating scene. 
                Our TSA-Net achieves the Spearman's Rank Correlation of 0.8476 and 0.9393 on AQA-7 and MTL-AQA, respectively, which are the new state-of-the-art results. The results on FR-FS also 
                verify the effectiveness of the TSA-Net. The code and FR-FS dataset are publicly available at https://github.com/Shunli-Wang/TSA-Net.</div>
        </div>
    </div>

    <div class="publication">
        <div class="pubimg"><img src="publications/img/slwang_AQA-Survey.png" class="thumbnail"></div>
        <div style="display:table-row">
            <div class="pubtitle">A Survey of Video-based Action Quality Assessment </div>
            <div class="pubauthors"><b>Shunli Wang</b>, Dingkang Yang, Peng Zhai, Qing Yu, Tao Suo, Zhan Sun, Ka Li, Lihua Zhang*</div>
            <div class="pubinfo">International Conference on Networking Systems of AI (INSAI 2021) (Oral) </div>
            <div class="pubbox trigger"><a href="javascript:void(0)">Abstract</a></div>
            <div class="pubbox"><a href="publications/pdf/slwang_AQA-Survey.pdf">PDF</a></div>
            <!-- <div class="pubbox"><a href=https://github.com/Shunli-Wang/TSA-Net>Code</a></div> -->
            <div class="pubbox"><a href="publications/pdf/slwang_AQA-Survey_slides.pdf">Slides</a></div>
            <!-- <div class="pubbox"><a href="publications/bibtex/TSA-Net.bib">BibTex</a></div> -->
            <div class="abstract">Human action recognition and analysis have great demand and important application significance in video surveillance, video retrieval, 
                and human-computer interaction. The task of human action quality evaluation requires the intelligent system to automatically and objectively evaluate the 
                action completed by the human. The action quality assessment model can reduce the human and material resources spent in action evaluation and reduce subjectivity. 
                In this paper, we provide a comprehensive survey of existing papers on video-based action quality assessment. Different from human action recognition, 
                the application scenario of action quality assessment is relatively narrow. Most of the existing work focuses on sports and medical care. We first introduce the 
                definition and challenges of human action quality assessment. Then we present the existing datasets and evaluation metrics. In addition, we summarized the methods 
                of sports and medical care according to the model categories and publishing institutions according to the characteristics of the two fields. At the end, combined 
                with recent work, the promising development direction in action quality assessment is discussed.</div>
        </div>
    </div>

    <div class="publication">
        <div class="pubimg"><img src="publications/img/slwang_PIM Accelerator.png" class="thumbnail"></div>
        <div style="display:table-row">
            <div class="pubtitle">A 0.57-GOPS/DSP Object Detection PIM Accelerator on FPGA </div>
            <div class="pubauthors">Bo Jiao, Jinshan Zhang, Yuanyuan Xie, <b>Shunli Wang</b>, Haozhe Zhu, Xiaoyang Kang, Zhiyan Dong, Lihua Zhang, Chixiao Chen</div>
            <div class="pubinfo">Proceedings of the 26th Asia and South Pacific Design Automation Conference (ASP-DAC '21) </div>
            <div class="pubbox trigger"><a href="javascript:void(0)">Abstract</a></div>
            <div class="pubbox"><a href="publications/pdf/slwang_PIM_Accelerator_on_FPGA.pdf">PDF</a></div>
            <div class="pubbox"><a href="publications/bibtex/slwang_PIM_Accelerator.bib">BibTex</a></div>
            <div class="abstract">The paper presents an object detection accelerator featuring a processing-in-memory (PIM) architecture on FPGAs. PIM architectures are well known for their energy 
                efficiency and avoidance of the memory wall. In the accelerator, a PIM unit is developed using BRAM and LUT based counters, which also helps to improve the DSP performance density. 
                The overall architecture consists of 64 PIM units and three memory buffers to store inter-layer results. A shrunk and quantized Tiny-YOLO network is mapped to the PIM accelerator, 
                where DRAM access is fully eliminated during inference. The design achieves a throughput of 201.6 GOPs at 100MHz clock rate and correspondingly, a performance density of 0.57 GOPS/DSP.</div>
        </div>
    </div>

    <div class="publication">
        <div class="pubimg"><img src="publications/img/slwang_Chiplet.png" class="thumbnail"></div>
        <div style="display:table-row">
            <div class="pubtitle">Computing Utilization Enhancement for Chiplet-based Homogeneous Processing-in-Memory Deep Learning Processors </div>
            <div class="pubauthors">Bo Jiao, Haozhe Zhu, Jinshan Zhang, <b>Shunli Wang</b>, Xiaoyang Kang, Lihua Zhang, Mingyu Wang and Chixiao Chen</div>
            <div class="pubinfo">Proceedings of the 2021 on Great Lakes Symposium on VLSI (GLSVLSI '21)</div>
            <div class="pubbox trigger"><a href="javascript:void(0)">Abstract</a></div>
            <div class="pubbox"><a href="publications/pdf/slwang_chiplet.pdf">PDF</a></div>
            <div class="pubbox"><a href=https://dlnext.acm.org/action/downloadSupplement?doi=10.1145%2F3453688.3461499&file=GLSVLSI2021-glsv073.mp4>Video</a></div>
            <div class="pubbox"><a href="publications/bibtex/slwang_Chiplet.bib">BibTex</a></div>
            <div class="abstract">This paper presents a design strategy of chiplet-based processing-in-memory systems for deep neural network applications. Monolithic silicon chips are area and power limited, failing 
                to catch the recent rapid growth of deep learning algorithms. The paper first demonstrates a straightforward layer-wise method that partitions the workload of a monolithic accelerator to a 
                multi-chiplet pipeline. A quantitative analysis shows that the straightforward separation degrades the overall utilization of computing resources due to the reduced on-chiplet memory size, 
                thus introducing a higher memory wall. A tile interleaving strategy is proposed to overcome such degradation. This strategy can segment one layer to different chiplets which maximizes the computing 
                utilization. To facilitate the strategy, the modification of the chiplet system hardware is also discussed. To validate the proposed strategy, a nine-chiplet processing-in-memory system is evaluated 
                with a custom-designed object detection network. Each chiplet can achieve a peak performance of 204.8GOPS at a 100-MHz rate. The peak performance of the overall system is 1.711TOPS, where no off-chip 
                memory access is needed. By the tile interleaving strategy, the utilization is improved from 53.9 to 92.8.</div>
        </div>
    </div>

    <!-- PUBLICATIONS -->
    <a name="projects">
        <div class="headline">Projects</div>


    <!-- EDUCATION -->
    <a name="education">
    <div class="headline">Education/Work</div>

    <!-- timeline -->
    <div class="timeline">
        <!-- entry -->
        <div class="entry">
            <div class="title">
                <p class="textbf">Sep 2019 - current</p>
                <p>PhD Student at Academy for Engineering and Technology, Fudan University, Shanghai<p>
            </div>
        <div class="body">
            <p>Research Topics: Fine-grained action recognition and action quality assessment in medical scenes.</p>
        </div>
        <!-- entry -->
        <div class="entry">
            <div class="title">
                <p class="textbf">Sep 2015 - Jun 2019</p>
                <p>Bachelor's Degree, Anhui University<p>
            </div>
        <div class="body">
            Thesis: Figure Skating Analysis System Based on Multi-target Tracking and Posture Estimation
        </div>
        
    </div>


</div>

</body>
